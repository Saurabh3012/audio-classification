{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature(file_name):\n",
    "    X, sample_rate = librosa.load(file_name)\n",
    "#     print (\"Features :\",len(X), \"sampled at \", sample_rate, \"hz\")\n",
    "    stft = np.abs(librosa.stft(X))\n",
    "    mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T,axis=0)\n",
    "    chroma = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T,axis=0)\n",
    "    mel = np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n",
    "    contrast = np.mean(librosa.feature.spectral_contrast(S=stft, sr=sample_rate).T,axis=0)\n",
    "    tonnetz = np.mean(librosa.feature.tonnetz(y=librosa.effects.harmonic(X), sr=sample_rate).T,axis=0)\n",
    "    return np.hstack([mfccs,chroma,mel,contrast,tonnetz])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create custom dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.label = pd.read_csv(csv_file) \n",
    "        self.transform = transform\n",
    "        self.file_arr = np.asarray(self.label.iloc[:, 2])\n",
    "        self.label_arr = np.asarray(self.label.iloc[:, 3])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.label)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "#         print(self.label.iloc[idx])\n",
    "        file_name = os.path.join(self.root_dir, str(self.file_arr[idx])+\".wav\")\n",
    "        feature = extract_feature(file_name)\n",
    "#         feature = feature[0:192]\n",
    "#         feature = feature.reshape(1,192)\n",
    "        label = self.label_arr[idx]\n",
    "        if self.transform:\n",
    "            feature = self.transform(feature)\n",
    "        \n",
    "        \n",
    "        sample = {\n",
    "            'feature': feature,\n",
    "            'label': label,\n",
    "            'path': file_name\n",
    "        }\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = AudioDataset(csv_file='new_train.csv', root_dir=\"./Train/\")\n",
    "test_set = AudioDataset(csv_file='new_test.csv', root_dir=\"./Train/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(train_set, batch_size=1, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(test_set, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'feature': tensor([[[-2.7809e+02,  1.6555e+02,  3.2482e+01, -1.3510e+01, -2.8555e+01,\n",
      "          -2.9463e+01, -1.9443e+01,  1.1790e+00, -1.3600e+01, -9.4426e+00,\n",
      "          -6.8983e+00,  9.1908e+00,  1.4023e+01, -1.3031e+01, -3.7774e+00,\n",
      "           1.1573e+01,  8.5581e+00,  1.4359e+01,  2.5850e+01,  1.5572e+01,\n",
      "           5.7387e+00,  1.1703e+01,  4.1805e+00,  1.5850e+00,  1.0963e+01,\n",
      "           9.6809e-01,  2.6062e+00,  7.4083e+00,  5.0846e+00,  4.8101e+00,\n",
      "           5.5918e+00,  8.3724e+00,  2.0809e+00,  6.2174e+00,  6.0613e+00,\n",
      "           5.5879e+00,  6.6431e+00,  5.1613e+00,  7.0689e+00,  2.2142e+00,\n",
      "           3.5324e-01,  4.1810e-01,  4.6483e-01,  4.7792e-01,  5.4179e-01,\n",
      "           6.5794e-01,  7.4060e-01,  7.1677e-01,  6.0711e-01,  5.4696e-01,\n",
      "           4.4089e-01,  3.2264e-01,  3.1743e+01,  9.1086e+00,  1.6255e+00,\n",
      "           4.4560e-01,  7.4427e-02,  2.5994e-02,  6.3624e-02,  8.9661e-02,\n",
      "           1.4397e-01,  4.1701e-01,  1.2108e+00,  3.3454e+00,  1.0550e+01,\n",
      "           5.4820e+01,  1.3210e+02,  1.9385e+02,  4.8327e+02,  1.9004e+02,\n",
      "           1.0259e+01,  1.3447e+01,  2.4747e+01,  1.2892e+01,  9.0124e+00,\n",
      "           9.1873e+00,  9.8300e+00,  1.9420e+01,  3.2402e+01,  4.8183e+01,\n",
      "           5.9432e+01,  4.8000e+01,  1.1052e+01,  4.9421e+00,  3.8452e+00,\n",
      "           3.9436e+00,  3.0483e+00,  5.2111e-01,  2.3175e-01,  4.3952e-01,\n",
      "           4.9576e-01,  3.1080e-01,  4.0163e-01,  5.6335e-01,  8.6325e-01,\n",
      "           8.0730e-01,  1.0313e+00,  9.4877e-01,  5.0958e-01,  4.6297e-01,\n",
      "           3.3746e-01,  1.2071e-01,  5.3345e-02,  1.6405e-02,  1.3651e-02,\n",
      "           4.7235e-03,  5.7175e-03,  3.1600e-03,  2.7108e-03,  2.5362e-03,\n",
      "           2.8112e-03,  2.1698e-03,  3.0211e-03,  3.6738e-03,  2.7447e-03,\n",
      "           2.6660e-03,  4.6144e-03,  1.4867e-02,  3.7166e-02,  4.7631e-02,\n",
      "           3.6257e-02,  1.4353e-02,  1.1258e-02,  5.9626e-03,  4.2255e-03,\n",
      "           4.5703e-03,  9.7604e-03,  3.8656e-03,  6.1906e-04,  3.2400e-04,\n",
      "           8.9955e-05,  5.6401e-05,  1.4936e-04,  3.6449e-04,  1.8654e-03,\n",
      "           4.1207e-03,  3.6311e-03,  3.6820e-03,  4.6108e-03,  1.3388e-03,\n",
      "           2.2710e-04,  1.1681e-04,  9.2513e-05,  1.1589e-05,  1.1512e-05,\n",
      "           4.4381e-06,  4.4779e-05,  6.0256e-05,  4.0347e-05,  7.6259e-05,\n",
      "           2.1975e-05,  1.2290e-04,  1.1135e-04,  2.3445e-05,  6.2155e-05,\n",
      "           1.0626e-04,  8.5571e-05,  7.8637e-04,  2.1269e-03,  1.4677e-03,\n",
      "           8.9337e-04,  6.9847e-04,  2.6983e-04,  1.6067e-04,  1.0102e-04,\n",
      "           1.4022e-04,  1.7068e-04,  1.7186e-04,  1.8597e-04,  6.4873e-05,\n",
      "           1.7003e-05,  1.1659e-06,  7.7107e-07,  4.0264e-07,  9.9633e-07,\n",
      "           4.0789e-06,  9.5497e-07,  7.8760e-07,  5.2615e-06,  4.7825e-06,\n",
      "           2.9331e+01,  1.9332e+01,  1.6768e+01,  2.3836e+01,  2.6835e+01,\n",
      "           2.8602e+01,  2.5015e+01, -1.9586e-02,  2.4021e-02, -5.0159e-02,\n",
      "          -6.7730e-03,  1.4165e-02]]], dtype=torch.float64), 'path': ['./data/Train/6923.wav'], 'label': tensor([3])}\n",
      "torch.Size([1, 1, 192])\n"
     ]
    }
   ],
   "source": [
    "for sample in trainloader:\n",
    "    print(sample)\n",
    "    print(sample[\"feature\"].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim, n_layers, drop_prob=0.5):\n",
    "        \"\"\"\n",
    "        Initialize the model by setting up the layers.\n",
    "        \"\"\"\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "#         print(input_size)\n",
    "\n",
    "        # embedding and LSTM layers\n",
    "#         self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(input_size, hidden_dim, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "\n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "        # linear and sigmoid layers\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of our model on some input and hidden state.\n",
    "        \"\"\"\n",
    "        print(x.shape)\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "#         x = x.view()\n",
    "        # embeddings and lstm_out\n",
    "#         embeds = self.embedding(x)\n",
    "\n",
    "#         x = x.view(4, batch_size, 48)\n",
    "        \n",
    "        lstm_out, hidden = self.lstm(x, hidden)\n",
    "#         print(lstm_out.shape)\n",
    "\n",
    "        # stack up lstm outputs\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "\n",
    "        # dropout and fully-connected layer\n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        # sigmoid function\n",
    "        sig_out = self.logsoftmax(out)\n",
    "\n",
    "        # reshape to be batch_size first\n",
    "#         sig_out = sig_out.view(batch_size, -1)\n",
    "#         sig_out = sig_out[:, -1] # get last batch of labels\n",
    "\n",
    "        # return last sigmoid output and hidden state\n",
    "        return sig_out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "\n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class RNN(nn.Module):\n",
    "#     def __init__(self, input_size, output_size, hidden_dim, n_layers):\n",
    "#         super(RNN, self).__init__()\n",
    "        \n",
    "#         self.hidden_dim=hidden_dim\n",
    "        \n",
    "#         self.rnn = nn.RNN(input_size, hidden_dim, n_layers, batch_first=True)\n",
    "        \n",
    "#         self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        \n",
    "#     def forward(self, x, hidden):\n",
    "        \n",
    "#         batch_size = x.size(0)\n",
    "        \n",
    "#         r_out, hidden = self.rnn(x, hidden)\n",
    "        \n",
    "#         r_out = r_out.view(-1, self.hidden_dim)\n",
    "        \n",
    "#         output = self.fc(r_out)\n",
    "        \n",
    "#         return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (rnn): RNN(193, 256, num_layers=3, batch_first=True)\n",
       "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For simple RNN\n",
    "# input_size = 193\n",
    "# output_size = 1\n",
    "# hidden_dim = 256\n",
    "# n_layers = 3\n",
    "\n",
    "# model = RNN(input_size, output_size, hidden_dim, n_layers)\n",
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (lstm): LSTM(193, 256, num_layers=3, batch_first=True, dropout=0.5)\n",
       "  (dropout): Dropout(p=0.3)\n",
       "  (fc): Linear(in_features=256, out_features=10, bias=True)\n",
       "  (logsoftmax): LogSoftmax()\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For LSTM\n",
    "input_size = 193\n",
    "output_size = 10\n",
    "hidden_dim = 256\n",
    "n_layers = 3\n",
    "\n",
    "model = RNN(input_size, output_size, hidden_dim, n_layers)\n",
    "model = model.double()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_on_gpu=torch.cuda.is_available()\n",
    "lr = 0.001\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yesd\n",
      "torch.Size([1, 1, 193])\n",
      "yes2\n",
      "tensor([[-2.2408, -2.2962, -2.3131, -2.2686, -2.3215, -2.3580, -2.2872, -2.2588,\n",
      "         -2.3285, -2.3606]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor(2.2588, device='cuda:0', dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "yes3\n",
      "torch.Size([1, 1, 193])\n",
      "tensor([[-2.2379, -2.3028, -2.3144, -2.2757, -2.3253, -2.3450, -2.2738, -2.2547,\n",
      "         -2.3337, -2.3705]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "torch.Size([1, 1, 193])\n",
      "tensor([[-2.2279, -2.3013, -2.3173, -2.2772, -2.3296, -2.3454, -2.2716, -2.2565,\n",
      "         -2.3309, -2.3774]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "torch.Size([1, 1, 193])\n",
      "tensor([[-2.2208, -2.3010, -2.3144, -2.2814, -2.3348, -2.3483, -2.2664, -2.2605,\n",
      "         -2.3274, -2.3809]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "torch.Size([1, 1, 193])\n",
      "tensor([[-2.2148, -2.3006, -2.3116, -2.2857, -2.3392, -2.3480, -2.2618, -2.2658,\n",
      "         -2.3304, -2.3786]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "torch.Size([1, 1, 193])\n",
      "tensor([[-2.2115, -2.2976, -2.3108, -2.2857, -2.3444, -2.3531, -2.2591, -2.2639,\n",
      "         -2.3343, -2.3770]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "torch.Size([1, 1, 193])\n",
      "tensor([[-2.2060, -2.2981, -2.3103, -2.2888, -2.3491, -2.3531, -2.2579, -2.2645,\n",
      "         -2.3357, -2.3745]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<LogSoftmaxBackward>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-0cd8edd07dcc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtestloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m                 \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"feature\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m                 \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-a263511debf7>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m#         print(self.label.iloc[idx])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mfile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_arr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".wav\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mfeature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;31m#         feature = feature[0:192]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m#         feature = feature.reshape(1,192)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-86c9794792df>\u001b[0m in \u001b[0;36mextract_feature\u001b[0;34m(file_name)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mextract_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#     print (\"Features :\",len(X), \"sampled at \", sample_rate, \"hz\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mstft\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlibrosa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmfccs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlibrosa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmfcc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_mfcc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/librosa/core/audio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msr\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr_native\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mres_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/librosa/core/audio.py\u001b[0m in \u001b[0;36mresample\u001b[0;34m(y, orig_sr, target_sr, res_type, fix, scale, **kwargs)\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m         \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresampy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_sr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_sr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mres_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfix\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/resampy/core.py\u001b[0m in \u001b[0;36mresample\u001b[0;34m(x, sr_orig, sr_new, axis, filter, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0mx_2d\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswapaxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0my_2d\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswapaxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m     \u001b[0mresample_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_2d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_2d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_ratio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterp_win\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterp_delta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "counter = 0\n",
    "print_every = 100\n",
    "clip = 5\n",
    "batch_size = 1\n",
    "\n",
    "if train_on_gpu:\n",
    "    model.cuda()\n",
    "model.train()\n",
    "\n",
    "for e in range(epochs):\n",
    "    h = model.init_hidden(batch_size)\n",
    "    \n",
    "    for sample in trainloader:\n",
    "        inputs = sample[\"feature\"]\n",
    "        labels = sample[\"label\"]\n",
    "        \n",
    "        inputs = inputs.unsqueeze(0)\n",
    "\n",
    "        \n",
    "        if train_on_gpu:\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        \n",
    "        h = tuple([each.data for each in h])\n",
    "        \n",
    "        model.zero_grad()\n",
    "        \n",
    "        print(\"yesd\")\n",
    "        \n",
    "        output, h = model(inputs, h)\n",
    "        \n",
    "        print(\"yes2\")\n",
    "        \n",
    "        \n",
    "        print(output)\n",
    "#         output = (output.unsqueeze(0))\n",
    "#         labels = (labels.unsqueeze(0))\n",
    "        \n",
    "        loss = criterion(output, labels)\n",
    "    \n",
    "        print(loss)\n",
    "    \n",
    "        loss.backward()\n",
    "        \n",
    "        \n",
    "        print(\"yes3\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if counter % print_every == 0:\n",
    "            val_h = model.init_hidden(batch_size)\n",
    "            val_losses = []\n",
    "            model.eval()\n",
    "            \n",
    "            for sample in testloader:\n",
    "                inputs = sample[\"feature\"]\n",
    "                labels = sample[\"label\"]\n",
    "                \n",
    "                val_h = tuple([each.data for each in val_h])\n",
    "                \n",
    "                inputs = inputs.unsqueeze(0)\n",
    "\n",
    "                \n",
    "                if train_on_gpu:\n",
    "                    inputs, labels = inputs.cuda(), labels.cuda()\n",
    "                    \n",
    "                output, val_h = model(inputs, val_h)\n",
    "                \n",
    "                print(output)\n",
    "                val_loss = criterion(output, labels)\n",
    "                \n",
    "                val_losses.append(val_loss.item())\n",
    "            \n",
    "            model.train()\n",
    "            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                  \"Step: {}...\".format(counter),\n",
    "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saurabh-env",
   "language": "python",
   "name": "saurabh-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
